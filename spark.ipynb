{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34950b58-7af6-48e5-a09b-c3c3a140db73",
   "metadata": {},
   "source": [
    "### Loading Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ea1b46-a4ae-4e00-a1f1-779ade5ed4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as spark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5106ba4-3c79-4acd-a49a-98c8d7e0b9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/01 03:34:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+--------+-------------+------------+------+--------------------+---------+\n",
      "| id|   last_name|               email|  gender|   department|  start_date|salary|           job_title|region_id|\n",
      "+---+------------+--------------------+--------+-------------+------------+------+--------------------+---------+\n",
      "|  1|    'Kelley'|'rkelley0@soundcl...|'Female'|  'Computers'| '10/2/2009'| 67470|'Structural Engin...|        2|\n",
      "|  2| 'Armstrong'|'sarmstrong1@info...|  'Male'|     'Sports'| '3/31/2008'| 71869| 'Financial Advisor'|        2|\n",
      "|  3|      'Carr'|'fcarr2@woothemes...|  'Male'| 'Automotive'| '7/12/2009'|101768|'Recruiting Manager'|        3|\n",
      "|  4|    'Murray'|   'jmurray3@gov.uk'|'Female'|   'Jewelery'|'12/25/2014'| 96897|'Desktop Support ...|        3|\n",
      "|  5|     'Ellis'|'jellis4@scienced...|'Female'|    'Grocery'| '9/19/2002'| 63702|'Software Enginee...|        7|\n",
      "|  6|  'Phillips'|'bphillips5@time....|  'Male'|      'Tools'| '8/21/2013'|118497|'Executive Secret...|        1|\n",
      "|  7|'Williamson'|'rwilliamson6@ted...|  'Male'|  'Computers'| '5/14/2006'| 65889|  'Dental Hygienist'|        6|\n",
      "|  8|    'Harris'| 'aharris7@ucoz.com'|'Female'|       'Toys'| '8/12/2003'| 84427|'Safety Technicia...|        4|\n",
      "|  9|     'James'|'rjames8@prnewswi...|  'Male'|   'Jewelery'|  '9/7/2005'|108657|   'Sales Associate'|        2|\n",
      "| 10|   'Sanchez'|'rsanchez9@cloudf...|  'Male'|     'Movies'| '3/13/2013'|108093|'Sales Representa...|        1|\n",
      "| 11|    'Jacobs'|'jjacobsa@sbwire....|'Female'|   'Jewelery'|'11/27/2003'|121966|'Community Outrea...|        7|\n",
      "| 12|     'Black'|'mblackb@edublogs...|  'Male'|   'Clothing'|  '2/4/2003'| 44179|   'Data Coordiator'|        7|\n",
      "| 13|   'Schmidt'|'sschmidtc@state....|  'Male'|       'Baby'|'10/13/2002'| 85227|'Compensation Ana...|        3|\n",
      "| 14|      'Webb'|  'awebbd@baidu.com'|'Female'|  'Computers'|'10/22/2006'| 59763|'Software Test En...|        4|\n",
      "| 15|    'Jacobs'|'ajacobse@google.it'|'Female'|      'Games'|  '3/4/2007'|141139|'Community Outrea...|        7|\n",
      "| 16|    'Medina'|'smedinaf@amazona...|'Female'|       'Baby'| '3/14/2008'|106659| 'Web Developer III'|        1|\n",
      "| 17|    'Morgan'|'dmorgang@123-reg...|'Female'|       'Kids'|  '5/4/2011'|148952|     'Programmer IV'|        6|\n",
      "| 18|    'Nguyen'|'jnguyenh@google....|  'Male'|       'Home'| '11/3/2014'| 93804|      'Geologist II'|        5|\n",
      "| 19|       'Day'|'rdayi@chronoengi...|  'Male'|'Electronics'| '9/22/2004'|109890|          'VP Sales'|        3|\n",
      "| 20|      'Carr'|  'dcarrj@ocn.ne.jp'|'Female'|     'Movies'|'11/22/2007'|115274|'VP Quality Control'|        5|\n",
      "+---+------------+--------------------+--------+-------------+------------+------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the CSV file\n",
    "emp_df = spark.read.csv(\"./data/employee.txt\", header=True)\n",
    "\n",
    "# Show the dataframe contents\n",
    "emp_df.show()\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "features_df = spark.createDataFrame([\n",
    "    (1,Vectors.dense([10.0,10000.0, 1.0]),),\n",
    "    (2,Vectors.dense([20.0,30000.0, 2.0]),),\n",
    "    (3,Vectors.dense([30.0,40000.0, 3.0]),)\n",
    "],[\"id\",\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d77a1b0b-897f-4ece-a1b5-608f7164e97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', StringType(), True), StructField('last_name', StringType(), True), StructField('email', StringType(), True), StructField('gender', StringType(), True), StructField('department', StringType(), True), StructField('start_date', StringType(), True), StructField('salary', StringType(), True), StructField('job_title', StringType(), True), StructField('region_id', StringType(), True)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b14ce7-3e38-45da-94d4-8abc002b3c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- region_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e779ca2b-bc27-4db4-b76e-b6f89d68ca30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='1', last_name=\"'Kelley'\", email=\"'rkelley0@soundcloud.com'\", gender=\"'Female'\", department=\"'Computers'\", start_date=\"'10/2/2009'\", salary='67470', job_title=\"'Structural Engineer'\", region_id='2'),\n",
       " Row(id='2', last_name=\"'Armstrong'\", email=\"'sarmstrong1@infoseek.co.jp'\", gender=\"'Male'\", department=\"'Sports'\", start_date=\"'3/31/2008'\", salary='71869', job_title=\"'Financial Advisor'\", region_id='2'),\n",
       " Row(id='3', last_name=\"'Carr'\", email=\"'fcarr2@woothemes.com'\", gender=\"'Male'\", department=\"'Automotive'\", start_date=\"'7/12/2009'\", salary='101768', job_title=\"'Recruiting Manager'\", region_id='3'),\n",
       " Row(id='4', last_name=\"'Murray'\", email=\"'jmurray3@gov.uk'\", gender=\"'Female'\", department=\"'Jewelery'\", start_date=\"'12/25/2014'\", salary='96897', job_title=\"'Desktop Support Technician'\", region_id='3'),\n",
       " Row(id='5', last_name=\"'Ellis'\", email=\"'jellis4@sciencedirect.com'\", gender=\"'Female'\", department=\"'Grocery'\", start_date=\"'9/19/2002'\", salary='63702', job_title=\"'Software Engineer III'\", region_id='7')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da2bdc84-4079-401f-8512-887e01e152fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7426da79-0207-4b69-8eea-a685a4833d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = emp_df.sample(False,0.1)\n",
    "sample_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2eeeff-9b4d-4d6d-8d23-fabc52f699b9",
   "metadata": {},
   "source": [
    "#### Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe0a04e-3801-4e41-94dd-c3e3fb5cb88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "379b616b-9301-4e68-9c78-f360fca7027f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "381a8d4f-f5cf-4f44-888c-750f2f9734cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), sfeatures=SparseVector(3, {}))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_scaler = MinMaxScaler(inputCol=\"features\",outputCol=\"sfeatures\")\n",
    "# feature_scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"sfeatures\", handleInvalid=\"keep\")\n",
    "smodel = feature_scaler.fit(features_df)\n",
    "sfeatures_df = smodel.transform(features_df)\n",
    "sfeatures_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c50e45-d437-4444-8233-e022573e9ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------------+\n",
      "|features          |sfeatures                   |\n",
      "+------------------+----------------------------+\n",
      "|[10.0,10000.0,1.0]|(3,[],[])                   |\n",
      "|[20.0,30000.0,2.0]|[0.5,0.6666666666666667,0.5]|\n",
      "|[30.0,40000.0,3.0]|[1.0,1.0,1.0]               |\n",
      "+------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfeatures_df.select(\"features\", \"sfeatures\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c40d135-b2b2-4dd1-902f-3476614cff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------------+\n",
      "|features          |sfeatures                   |\n",
      "+------------------+----------------------------+\n",
      "|[10.0,10000.0,1.0]|[0.0,0.0,0.0]               |\n",
      "|[20.0,30000.0,2.0]|[0.5,0.6666666666666667,0.5]|\n",
      "|[30.0,40000.0,3.0]|[1.0,1.0,1.0]               |\n",
      "+------------------+----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT, DenseVector\n",
    "\n",
    "# Create UDF to convert sparse vectors to dense\n",
    "to_dense = udf(lambda v: DenseVector(v.toArray()), VectorUDT())\n",
    "\n",
    "# Apply the conversion\n",
    "sfeatures_df = sfeatures_df.withColumn(\"sfeatures\", to_dense(\"sfeatures\"))\n",
    "\n",
    "# Show results\n",
    "sfeatures_df.select(\"features\", \"sfeatures\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c4407c-0273-49f6-9c93-eee128855c37",
   "metadata": {},
   "source": [
    "#### Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a623f97-0c57-484b-ba8c-4a0d5646466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "feature_stand_scaler = StandardScaler(inputCol=\"features\",outputCol=\"sfeatures\",withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e55fe225-dd32-437e-831b-afa39b39a353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), sfeatures=DenseVector([-1.0, -1.0911, -1.0]))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stand_smodel = feature_stand_scaler.fit(features_df)\n",
    "stand_sfeatures_df = stand_smodel.transform(features_df)\n",
    "stand_sfeatures_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db8a4d9d-84f6-47f8-abf6-142fe95f2d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+\n",
      "| id|          features|           sfeatures|\n",
      "+---+------------------+--------------------+\n",
      "|  1|[10.0,10000.0,1.0]|[-1.0,-1.09108945...|\n",
      "|  2|[20.0,30000.0,2.0]|[0.0,0.2182178902...|\n",
      "|  3|[30.0,40000.0,3.0]|[1.0,0.8728715609...|\n",
      "+---+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stand_sfeatures_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482badf1-6f54-48f2-a991-8a71788fd909",
   "metadata": {},
   "source": [
    "#### Bucketize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fdef522-fa77-426e-83c5-db0c0af716cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|features|\n",
      "+--------+\n",
      "|  -800.0|\n",
      "|   -10.5|\n",
      "|    -1.7|\n",
      "|     0.0|\n",
      "|     8.2|\n",
      "|    90.1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -10.0, 0.0, 10.0, float(\"inf\")]\n",
    "b_data = [(-800.0,),(-10.5,),(-1.7,),(0.0,),(8.2,),(90.1,)]\n",
    "b_df = spark.createDataFrame(b_data, [\"features\"])\n",
    "b_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a50e4e5e-d386-488b-ae7f-817574b41f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|features|bfeatures|\n",
      "+--------+---------+\n",
      "|  -800.0|      0.0|\n",
      "|   -10.5|      0.0|\n",
      "|    -1.7|      1.0|\n",
      "|     0.0|      2.0|\n",
      "|     8.2|      2.0|\n",
      "|    90.1|      3.0|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\",outputCol=\"bfeatures\")\n",
    "bucketed_df = bucketizer.transform(b_df)\n",
    "bucketed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1cd43a-9b3a-4c8e-82e5-24c089d944bd",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0812bb0-e9e8-421b-b270-25db63533352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e512f4d3-45fb-4b59-be17-fa93294e17f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  1|Norah is doing so...|\n",
      "|  2|The slick sly fox...|\n",
      "|  3|I am going to tok...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_df = spark.createDataFrame([\n",
    "    (1, \"Norah is doing some stuff with Spark MLlib\"),\n",
    "    (2, \"The slick sly fox jumped over the lazy brown dog\"),\n",
    "    (3, \"I am going to tokenize some text in a second here\")],\n",
    "    [\"id\",\"sentence\"])\n",
    "\n",
    "sentences_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72d575b0-eb04-4058-beab-fab255d82cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|Norah is doing so...|[norah, is, doing...|\n",
      "|  2|The slick sly fox...|[the, slick, sly,...|\n",
      "|  3|I am going to tok...|[i, am, going, to...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_token = Tokenizer(inputCol=\"sentence\",outputCol=\"words\")\n",
    "sent_tokenized_df = sent_token.transform(sentences_df)\n",
    "sent_tokenized_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e10f97-0d00-494a-be66-b740bdf6e296",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84b8e490-aa2d-481d-94d3-9df657a17dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, sentence: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37ee08e4-c58b-49da-b34d-5bffff39c967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='Norah is doing some stuff with Spark MLlib')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4433b482-67d8-4bd1-b969-50ae9a822d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='Norah is doing some stuff with Spark MLlib', words=['norah', 'is', 'doing', 'some', 'stuff', 'with', 'spark', 'mllib'])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenized_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f39a8b36-c7bc-4754-81e3-5eff9a56dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\",outputCol=\"rawFeatures\",numFeatures=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab6d5a26-3e7b-4ccf-a384-715a316f70d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='Norah is doing some stuff with Spark MLlib', words=['norah', 'is', 'doing', 'some', 'stuff', 'with', 'spark', 'mllib'], rawFeatures=SparseVector(20, {0: 1.0, 2: 1.0, 6: 1.0, 8: 1.0, 9: 2.0, 10: 1.0, 15: 1.0}))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_hfTF_df = hashingTF.transform(sent_tokenized_df)\n",
    "sent_hfTF_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41c40c15-48b8-4874-a902-5330735369bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\",outputCol=\"idf_features\")\n",
    "idfModel = idf.fit(sent_hfTF_df)\n",
    "tfidf_df = idfModel.transform(sent_hfTF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45ebab3e-5de6-4d82-94d7-e319ba47f3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='Norah is doing some stuff with Spark MLlib', words=['norah', 'is', 'doing', 'some', 'stuff', 'with', 'spark', 'mllib'], rawFeatures=SparseVector(20, {0: 1.0, 2: 1.0, 6: 1.0, 8: 1.0, 9: 2.0, 10: 1.0, 15: 1.0}), idf_features=SparseVector(20, {0: 0.2877, 2: 0.6931, 6: 0.6931, 8: 0.0, 9: 0.5754, 10: 0.2877, 15: 0.2877}))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc1a5e-093b-47a5-8cd5-baad31e97248",
   "metadata": {},
   "source": [
    "#### Clustering\n",
    "- K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b778fed-5cbd-4667-8666-632e441f3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "cluster_df = spark.read.csv(\"./data/clustering_dataset.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ca703b5-146a-47ec-ab87-811ac1a586dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col1: int, col2: int, col3: int]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cabc5c0a-3363-45af-8938-45060e00f813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   7|   4|   1|\n",
      "|   7|   7|   9|\n",
      "|   7|   9|   6|\n",
      "|   1|   6|   5|\n",
      "|   6|   7|   7|\n",
      "|   7|   9|   4|\n",
      "|   7|  10|   6|\n",
      "|   7|   8|   2|\n",
      "|   8|   3|   8|\n",
      "|   4|  10|   5|\n",
      "|   7|   4|   5|\n",
      "|   7|   8|   4|\n",
      "|   2|   5|   1|\n",
      "|   2|   6|   2|\n",
      "|   2|   3|   8|\n",
      "|   3|   9|   1|\n",
      "|   4|   2|   9|\n",
      "|   1|   7|   1|\n",
      "|   6|   2|   3|\n",
      "|   4|   1|   9|\n",
      "+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ef6dc4e-69bc-4eee-8d08-8fe41da62a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------------+\n",
      "|col1|col2|col3|      features|\n",
      "+----+----+----+--------------+\n",
      "|   7|   4|   1| [7.0,4.0,1.0]|\n",
      "|   7|   7|   9| [7.0,7.0,9.0]|\n",
      "|   7|   9|   6| [7.0,9.0,6.0]|\n",
      "|   1|   6|   5| [1.0,6.0,5.0]|\n",
      "|   6|   7|   7| [6.0,7.0,7.0]|\n",
      "|   7|   9|   4| [7.0,9.0,4.0]|\n",
      "|   7|  10|   6|[7.0,10.0,6.0]|\n",
      "|   7|   8|   2| [7.0,8.0,2.0]|\n",
      "|   8|   3|   8| [8.0,3.0,8.0]|\n",
      "|   4|  10|   5|[4.0,10.0,5.0]|\n",
      "|   7|   4|   5| [7.0,4.0,5.0]|\n",
      "|   7|   8|   4| [7.0,8.0,4.0]|\n",
      "|   2|   5|   1| [2.0,5.0,1.0]|\n",
      "|   2|   6|   2| [2.0,6.0,2.0]|\n",
      "|   2|   3|   8| [2.0,3.0,8.0]|\n",
      "|   3|   9|   1| [3.0,9.0,1.0]|\n",
      "|   4|   2|   9| [4.0,2.0,9.0]|\n",
      "|   1|   7|   1| [1.0,7.0,1.0]|\n",
      "|   6|   2|   3| [6.0,2.0,3.0]|\n",
      "|   4|   1|   9| [4.0,1.0,9.0]|\n",
      "+----+----+----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=[\"col1\",\"col2\",\"col3\"],outputCol=\"features\")\n",
    "vcluster_df = vectorAssembler.transform(cluster_df)\n",
    "vcluster_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3d362b1-98a4-4906-bd8f-64e61f8d28ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 03:34:41 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans().setK(3)\n",
    "kmeans = kmeans.setSeed(1)\n",
    "kmodel = kmeans.fit(vcluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c47b6f17-7773-4cb2-816d-5b4e68685a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([35.88461538, 31.46153846, 34.42307692]),\n",
       " array([80.        , 79.20833333, 78.29166667]),\n",
       " array([5.12, 5.84, 4.84])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers = kmodel.clusterCenters()\n",
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda2a425-7b0e-4143-b7df-ad3d4fbe4fb2",
   "metadata": {},
   "source": [
    "- Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c698c55f-383c-45a4-b96b-a7ac3e64152e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------------+\n",
      "|col1|col2|col3|      features|\n",
      "+----+----+----+--------------+\n",
      "|   7|   4|   1| [7.0,4.0,1.0]|\n",
      "|   7|   7|   9| [7.0,7.0,9.0]|\n",
      "|   7|   9|   6| [7.0,9.0,6.0]|\n",
      "|   1|   6|   5| [1.0,6.0,5.0]|\n",
      "|   6|   7|   7| [6.0,7.0,7.0]|\n",
      "|   7|   9|   4| [7.0,9.0,4.0]|\n",
      "|   7|  10|   6|[7.0,10.0,6.0]|\n",
      "|   7|   8|   2| [7.0,8.0,2.0]|\n",
      "|   8|   3|   8| [8.0,3.0,8.0]|\n",
      "|   4|  10|   5|[4.0,10.0,5.0]|\n",
      "|   7|   4|   5| [7.0,4.0,5.0]|\n",
      "|   7|   8|   4| [7.0,8.0,4.0]|\n",
      "|   2|   5|   1| [2.0,5.0,1.0]|\n",
      "|   2|   6|   2| [2.0,6.0,2.0]|\n",
      "|   2|   3|   8| [2.0,3.0,8.0]|\n",
      "|   3|   9|   1| [3.0,9.0,1.0]|\n",
      "|   4|   2|   9| [4.0,2.0,9.0]|\n",
      "|   1|   7|   1| [1.0,7.0,1.0]|\n",
      "|   6|   2|   3| [6.0,2.0,3.0]|\n",
      "|   4|   1|   9| [4.0,1.0,9.0]|\n",
      "+----+----+----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vcluster_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d982037a-bf35-4387-a729-44377cc3623e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5.12, 5.84, 4.84]),\n",
       " array([35.88461538, 31.46153846, 34.42307692]),\n",
       " array([80.        , 79.20833333, 78.29166667])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "bkmeans = BisectingKMeans().setK(3)\n",
    "kmeans = kmeans.setSeed(1)\n",
    "bkmodel = bkmeans.fit(vcluster_df)\n",
    "bkcenters = bkmodel.clusterCenters()\n",
    "bkcenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00857afd-00b8-4cba-9f99-87cd5811eff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([35.88461538, 31.46153846, 34.42307692]),\n",
       " array([80.        , 79.20833333, 78.29166667]),\n",
       " array([5.12, 5.84, 4.84])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1795d0-18a6-4487-a247-26ce736adaec",
   "metadata": {},
   "source": [
    "- Preprocessing Iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f3ea96e-d364-480a-a369-253bbe597274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=5.1, _c1=3.5, _c2=1.4, _c3=0.2, _c4='Iris-setosa')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "iris_df = spark.read.csv(\"./data/iris.txt\", inferSchema=True)\n",
    "iris_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17bbcb83-94bf-4b8f-a32c-aaac2430765c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, variety='Iris-setosa', features=DenseVector([5.1, 3.5, 1.4, 0.2]))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = iris_df.select(col(\"_c0\").alias(\"sepal_length\"),\n",
    "    col(\"_c1\").alias(\"sepal_width\"),\n",
    "    col(\"_c2\").alias(\"petal_length\"),\n",
    "    col(\"_c3\").alias(\"petal_width\"),\n",
    "    col(\"_c4\").alias(\"variety\"))\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"sepal_length\", \n",
    "                        \"sepal_width\",\"petal_length\", \n",
    "                        \"petal_width\"], outputCol=\"features\")\n",
    "\n",
    "virus_df = vectorAssembler.transform(iris_df)\n",
    "virus_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e64b6ae-faf5-48c2-b01d-556985143684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|    variety|         features|label|\n",
      "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|  0.0|\n",
      "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"variety\",outputCol=\"label\")\n",
    "ivirus_df = indexer.fit(virus_df).transform(virus_df)\n",
    "ivirus_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce612336-99f6-4c0b-b671-d0eae3f17b84",
   "metadata": {},
   "source": [
    "- Naive Bayes classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb486369-8293-46e6-9543-edbb3e07883d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, variety: string, features: vector, label: double]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ivirus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5274b0ac-7737-4c47-8f7e-e6dadb70f910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 98\n",
      "Test: 52\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "splits = ivirus_df.randomSplit([0.6,0.4],1)\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "\n",
    "print(f\"\"\"Train: {train_df.count()}\n",
    "Test: {test_df.count()}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35305b11-2ead-4f77-801f-17a4ffee5865",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(modelType=\"multinomial\")\n",
    "nbmodel = nb.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "007bb025-cdb5-4bc3-abc2-645924b9bc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=4.3, sepal_width=3.0, petal_length=1.1, petal_width=0.1, variety='Iris-setosa', features=DenseVector([4.3, 3.0, 1.1, 0.1]), label=0.0, rawPrediction=DenseVector([-9.9894, -11.3476, -11.902]), probability=DenseVector([0.7118, 0.183, 0.1051]), prediction=0.0)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = nbmodel.transform(test_df)\n",
    "predictions_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4900767f-eacb-41e5-81ed-14f9d4bf24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3079288d-2e32-42b7-8938-5f3777dc60d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9807692307692307"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbaccuracy = evaluator.evaluate(predictions_df)\n",
    "nbaccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50525f2c-ff20-4380-8595-185513f43700",
   "metadata": {},
   "source": [
    "- Multilayer perception classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f30e069c-2ff5-464c-8e9b-bc5af47c4bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, variety='Iris-setosa', features=DenseVector([5.1, 3.5, 1.4, 0.2]), label=0.0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ivirus_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e53e4a4-aacf-470a-8d89-46c5f054649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 98\n",
      "Test: 52\n",
      "Indexed vector: 150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Train: {train_df.count()}\n",
    "Test: {test_df.count()}\n",
    "Indexed vector: {ivirus_df.count()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "609c5957-574c-412a-9979-de735c6427a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "layers = [4,3] # < 4 measures + 3 types\n",
    "neurons = (5, 5, 5, 5, 5) # separately define our neurons here\n",
    "# 4s yield 0.6923\n",
    "# 9s yield 0.9807\n",
    "# 5 - 8 seems to work best here\n",
    "layers = [4,*neurons,3] # using *neurons to unpack dynamically\n",
    "mlp = MultilayerPerceptronClassifier(layers=layers,seed=1)\n",
    "mlp_model = mlp.fit(train_df)\n",
    "mlp_predictions = mlp_model.transform(test_df)\n",
    "\n",
    "mlp_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "mlp_accuracy = mlp_evaluator.evaluate(mlp_predictions)\n",
    "mlp_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d8d90-75f2-4aea-bccb-2398b3e426aa",
   "metadata": {},
   "source": [
    "- Decision trees classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbd79051-acdc-4bbd-b2a7-877b4c718ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9423076923076923"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Initialize Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "\n",
    "dt_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dt_accuracy = dt_evaluator.evaluate(dt_predictions)\n",
    "dt_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe737fc-610d-46d2-8e05-baa05dd52966",
   "metadata": {},
   "source": [
    "- Regression\n",
    "- Preprocessing regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3adc5cb7-3cb9-48fd-801d-22252def0b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AT: double, V: double, AP: double, RH: double, PE: double]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "pp_df = spark.read.csv(\"./data/ccpp.csv\",header=True,inferSchema=True)\n",
    "pp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0d9c7fb-67d3-410e-be02-99bab832a694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AT=14.96, V=41.76, AP=1024.07, RH=73.17, PE=463.26, features=DenseVector([14.96, 41.76, 1024.07, 73.17]))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"AT\",\"V\",\"AP\",\"RH\"],outputCol=\"features\")\n",
    "vpp_df = vectorAssembler.transform(pp_df)\n",
    "vpp_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2511ca46-7fdf-44e2-8948-623294b8b45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 03:34:54 WARN Instrumentation: [376c8718] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/02/01 03:34:55 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\",labelCol=\"PE\")\n",
    "lr_model = lr.fit(vpp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22025661-3652-491f-90ab-a016a19be439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-1.9775, -0.2339, 0.0621, -0.1581])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cb3c77e-264d-4a7f-96c4-8739f4f8bca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454.6092744523414"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bec168a4-fef2-4fac-986a-67c37016415b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.557126016749488"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8338d78-260d-4b36-9595-4b83b334e13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "lr_model.save(\"lr1.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626823be-a7d9-4d05-9c79-148dd4a4e310",
   "metadata": {},
   "source": [
    "- Decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e29acde-fda4-4c94-9283-0aef7f7c1f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AT=14.96, V=41.76, AP=1024.07, RH=73.17, PE=463.26)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "pp_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bac19506-1e2b-4d1e-a564-547ee8e490f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AT=14.96, V=41.76, AP=1024.07, RH=73.17, PE=463.26, features=DenseVector([14.96, 41.76, 1024.07, 73.17]))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=[\"AT\",\"V\",\"AP\",\"RH\"],outputCol=\"features\")\n",
    "vpp_df = vectorAssembler.transform(pp_df)\n",
    "vpp_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9bcd5cdb-df3e-4de1-a5c2-16ed80e8c681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6685\n",
      "Test: 2883\n",
      "Total: 9568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = vpp_df.randomSplit([0.7,0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "print(f\"\"\"Train: {train_df.count()}\n",
    "Test: {test_df.count()}\n",
    "Total: {vpp_df.count()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba77c3ea-18b4-444b-bc0a-88408476dcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.537880605370347"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol=\"features\",labelCol=\"PE\")\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(labelCol=\"PE\",predictionCol=\"prediction\",metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95c31b-03d7-4bb4-a550-c7bf5ec44e49",
   "metadata": {},
   "source": [
    "- Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0eca17bb-6ed2-4a5a-998e-d384d2460c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/spark/jars/spark-core_2.12-3.5.0.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.076993816516189"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"features\",labelCol=\"PE\")\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_evaluator = RegressionEvaluator(labelCol=\"PE\",predictionCol=\"prediction\",metricName=\"rmse\")\n",
    "gbt_rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "gbt_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f981c-74d2-4d57-bb2d-099acafaed27",
   "metadata": {},
   "source": [
    "- Collaborative filtering preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2444ca-2af0-46b3-97ed-4a3d1e02daf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
